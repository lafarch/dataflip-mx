# %% [markdown]
# # üó£Ô∏è DataFlip MX - Reddit Sentiment Analysis
# 
# **Objetivo:** Detectar quejas, necesidades y oportunidades en comunidades nicho
# 
# **Docs:** https://praw.readthedocs.io/

# %%
# === IMPORTAR LIBRER√çAS ===
import praw
import pandas as pd
import numpy as np
from datetime import datetime
import time
import re
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Librer√≠as importadas correctamente")

# %%
# === CONFIGURACI√ìN DE REDDIT API ===
# IMPORTANTE: Debes crear una app en https://www.reddit.com/prefs/apps

# Para este ejemplo, usamos credenciales de placeholder
# Reemplaza con tus credenciales reales en config/settings.py

try:
    from config.settings import REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT
    
    reddit = praw.Reddit(
        client_id=REDDIT_CLIENT_ID,
        client_secret=REDDIT_CLIENT_SECRET,
        user_agent=REDDIT_USER_AGENT
    )
    
    # Verificar conexi√≥n
    print(f"‚úÖ Conectado a Reddit como: {reddit.read_only}")
    
except Exception as e:
    print(f"‚ö†Ô∏è  Error conectando a Reddit: {e}")
    print("   Configura tus credenciales en config/settings.py")
    reddit = None

# %%
# === SUBREDDITS RELEVANTES ===
# Comunidades donde buscaremos oportunidades

SUBREDDITS = [
    'MechanicalKeyboards',  # Teclados mec√°nicos
    'gamecollecting',       # Coleccionistas de videojuegos
    'retrogaming',          # Gaming retro
    'AnalogCommunity',      # Fotograf√≠a anal√≥gica
    'Flipping',             # Comunidad de resellers
    'ThriftStoreHauls',     # Hallazgos en segunda mano
    'mexico',               # M√©xico general
    'ITAM',                 # ITAM (si existe)
]

# Keywords a buscar
KEYWORDS = [
    'calculadora financiera',
    'hp 12c',
    'game boy',
    'ipod',
    'teclado mecanico',
    'camara vintage',
    'segunda mano',
    'donde comprar',
    'recomendaciones',
]

print(f"üéØ {len(SUBREDDITS)} subreddits a analizar")
print(f"üîç {len(KEYWORDS)} keywords a buscar")

# %%
# === FUNCIONES DE SCRAPING ===

def search_subreddit(subreddit_name: str, query: str, limit: int = 100, time_filter: str = 'year'):
    """
    Busca posts en un subreddit espec√≠fico
    
    Args:
        subreddit_name: Nombre del subreddit
        query: T√©rmino de b√∫squeda
        limit: N√∫mero m√°ximo de posts
        time_filter: 'hour', 'day', 'week', 'month', 'year', 'all'
    
    Returns:
        Lista de diccionarios con datos de posts
    """
    if reddit is None:
        return []
    
    try:
        subreddit = reddit.subreddit(subreddit_name)
        posts = []
        
        for submission in subreddit.search(query, limit=limit, time_filter=time_filter):
            posts.append({
                'id': submission.id,
                'title': submission.title,
                'selftext': submission.selftext,
                'score': submission.score,
                'num_comments': submission.num_comments,
                'created_utc': datetime.fromtimestamp(submission.created_utc),
                'author': str(submission.author),
                'subreddit': subreddit_name,
                'url': submission.url,
                'permalink': f"https://reddit.com{submission.permalink}",
                'keyword': query
            })
        
        return posts
    
    except Exception as e:
        print(f"‚ùå Error en r/{subreddit_name} con '{query}': {e}")
        return []

def get_top_posts(subreddit_name: str, limit: int = 50, time_filter: str = 'month'):
    """
    Obtiene los posts m√°s populares de un subreddit
    """
    if reddit is None:
        return []
    
    try:
        subreddit = reddit.subreddit(subreddit_name)
        posts = []
        
        for submission in subreddit.top(limit=limit, time_filter=time_filter):
            posts.append({
                'id': submission.id,
                'title': submission.title,
                'selftext': submission.selftext[:500],  # Primeros 500 chars
                'score': submission.score,
                'num_comments': submission.num_comments,
                'created_utc': datetime.fromtimestamp(submission.created_utc),
                'author': str(submission.author),
                'subreddit': subreddit_name,
                'permalink': f"https://reddit.com{submission.permalink}"
            })
        
        return posts
    
    except Exception as e:
        print(f"‚ùå Error en r/{subreddit_name}: {e}")
        return []

# %%
# === AN√ÅLISIS: B√öSQUEDA POR KEYWORDS ===
if reddit:
    print("\n" + "="*60)
    print("üîç BUSCANDO KEYWORDS EN SUBREDDITS")
    print("="*60 + "\n")
    
    all_posts = []
    
    # Buscar solo en subreddits m√°s relevantes para no saturar
    priority_subreddits = ['Flipping', 'ThriftStoreHauls', 'mexico']
    priority_keywords = ['game boy', 'calculadora', 'vintage']
    
    for subreddit in priority_subreddits:
        for keyword in priority_keywords:
            print(f"üîç r/{subreddit} + '{keyword}'")
            posts = search_subreddit(subreddit, keyword, limit=25)
            
            if posts:
                all_posts.extend(posts)
                print(f"   ‚úÖ {len(posts)} posts encontrados")
            else:
                print(f"   ‚ö†Ô∏è  Sin resultados")
            
            time.sleep(2)  # Rate limiting
    
    if all_posts:
        df_posts = pd.DataFrame(all_posts)
        print(f"\n‚úÖ Total: {len(df_posts)} posts recopilados")
    else:
        print("\n‚ö†Ô∏è  No se encontraron posts")
        df_posts = pd.DataFrame()
else:
    print("\n‚ö†Ô∏è  Saltando an√°lisis (Reddit API no configurada)")
    df_posts = pd.DataFrame()

# %%
# === AN√ÅLISIS DE SENTIMIENTO B√ÅSICO ===
if not df_posts.empty:
    print("\n" + "="*60)
    print("üé≠ AN√ÅLISIS DE SENTIMIENTO (B√ÅSICO)")
    print("="*60 + "\n")
    
    # Palabras clave de necesidad/oportunidad
    oportunity_keywords = [
        'no encuentro', 'donde comprar', 'alguien sabe', 'recomendaciones',
        'busco', 'necesito', 'ayuda', 'd√≥nde', 'mejor', 'barato',
        'vale la pena', 'worth it', 'looking for', 'recommend', 'help'
    ]
    
    def detect_opportunity(text):
        """Detecta si un post expresa una necesidad/oportunidad"""
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in oportunity_keywords)
    
    # Combinar t√≠tulo y texto
    df_posts['full_text'] = df_posts['title'] + ' ' + df_posts['selftext'].fillna('')
    df_posts['is_opportunity'] = df_posts['full_text'].apply(detect_opportunity)
    
    # Filtrar oportunidades
    df_opportunities = df_posts[df_posts['is_opportunity'] == True].copy()
    
    print(f"üí° {len(df_opportunities)} posts identificados como OPORTUNIDADES")
    print(f"   ({len(df_opportunities)/len(df_posts)*100:.1f}% del total)\n")
    
    # Mostrar top oportunidades
    if not df_opportunities.empty:
        top_opps = df_opportunities.nlargest(10, 'score')[['title', 'score', 'num_comments', 'subreddit', 'permalink']]
        print("üî• Top 10 Oportunidades (por score):\n")
        for idx, row in top_opps.iterrows():
            print(f"üìå {row['title'][:80]}...")
            print(f"   ‚¨ÜÔ∏è  {row['score']} | üí¨ {row['num_comments']} | r/{row['subreddit']}")
            print(f"   üîó {row['permalink']}\n")

# %%
# === AN√ÅLISIS DE FRECUENCIA DE PALABRAS ===
if not df_posts.empty:
    print("\n" + "="*60)
    print("üìä PALABRAS M√ÅS FRECUENTES")
    print("="*60 + "\n")
    
    def clean_text(text):
        """Limpia y tokeniza texto"""
        # Lowercase y eliminar caracteres especiales
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text.lower())
        # Dividir en palabras
        words = text.split()
        # Filtrar stopwords b√°sicas
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
                     'of', 'is', 'it', 'that', 'this', 'with', 'as', 'be', 'are', 'i', 'my',
                     'de', 'la', 'el', 'en', 'y', 'que', 'por', 'para', 'con', 'un', 'una'}
        words = [w for w in words if len(w) > 3 and w not in stopwords]
        return words
    
    # Procesar todo el texto
    all_words = []
    for text in df_posts['full_text']:
        all_words.extend(clean_text(str(text)))
    
    # Contar frecuencias
    word_freq = Counter(all_words)
    top_words = pd.DataFrame(word_freq.most_common(30), columns=['palabra', 'frecuencia'])
    
    print(top_words.head(20))
    
    # Visualizar
    fig = px.bar(
        top_words.head(20),
        x='frecuencia',
        y='palabra',
        orientation='h',
        title='üìä Top 20 Palabras M√°s Mencionadas',
        labels={'frecuencia': 'Frecuencia', 'palabra': 'Palabra'},
        height=600,
        color='frecuencia',
        color_continuous_scale='Teal'
    )
    
    fig.update_layout(
        yaxis={'categoryorder':'total ascending'},
        template='plotly_white'
    )
    
    fig.show()

# %%
# === AN√ÅLISIS POR SUBREDDIT ===
if not df_posts.empty:
    print("\n" + "="*60)
    print("üìà AN√ÅLISIS POR SUBREDDIT")
    print("="*60 + "\n")
    
    subreddit_stats = df_posts.groupby('subreddit').agg({
        'id': 'count',
        'score': ['mean', 'max'],
        'num_comments': ['mean', 'max'],
        'is_opportunity': 'sum'
    }).round(2)
    
    subreddit_stats.columns = ['_'.join(col).strip() for col in subreddit_stats.columns.values]
    subreddit_stats = subreddit_stats.rename(columns={
        'id_count': 'total_posts',
        'score_mean': 'score_promedio',
        'score_max': 'score_max',
        'num_comments_mean': 'comentarios_promedio',
        'num_comments_max': 'comentarios_max',
        'is_opportunity_sum': 'oportunidades'
    })
    
    subreddit_stats = subreddit_stats.sort_values('oportunidades', ascending=False)
    
    print(subreddit_stats)
    
    # Visualizar
    fig = px.scatter(
        subreddit_stats.reset_index(),
        x='total_posts',
        y='oportunidades',
        size='score_promedio',
        color='comentarios_promedio',
        hover_name='subreddit',
        title='üéØ Subreddits: Posts vs Oportunidades',
        labels={
            'total_posts': 'Total de Posts',
            'oportunidades': 'Posts con Oportunidades',
            'score_promedio': 'Score Promedio',
            'comentarios_promedio': 'Comentarios Promedio'
        },
        height=500
    )
    
    fig.update_layout(template='plotly_white')
    fig.show()

# %%
# === TENDENCIAS TEMPORALES ===
if not df_posts.empty:
    print("\n" + "="*60)
    print("üìÖ ACTIVIDAD TEMPORAL")
    print("="*60 + "\n")
    
    # Agregar columna de mes
    df_posts['mes'] = df_posts['created_utc'].dt.to_period('M')
    
    # Contar posts por mes
    temporal = df_posts.groupby('mes').agg({
        'id': 'count',
        'is_opportunity': 'sum'
    }).rename(columns={'id': 'total_posts', 'is_opportunity': 'oportunidades'})
    
    temporal.index = temporal.index.to_timestamp()
    
    # Visualizar
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(
        x=temporal.index,
        y=temporal['total_posts'],
        name='Total Posts',
        line=dict(color='blue', width=2)
    ))
    
    fig.add_trace(go.Scatter(
        x=temporal.index,
        y=temporal['oportunidades'],
        name='Oportunidades',
        line=dict(color='red', width=2, dash='dash')
    ))
    
    fig.update_layout(
        title='üìÖ Actividad en Reddit a lo Largo del Tiempo',
        xaxis_title='Fecha',
        yaxis_title='N√∫mero de Posts',
        height=400,
        template='plotly_white',
        hovermode='x unified'
    )
    
    fig.show()

# %%
# === EXPORTAR DATOS ===
if not df_posts.empty:
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # Exportar posts completos
    df_posts.to_csv(f'data/processed/reddit_posts_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    # Exportar solo oportunidades
    if not df_opportunities.empty:
        df_opportunities.to_csv(f'data/analytics/reddit_opportunities_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    # Exportar estad√≠sticas de subreddits
    subreddit_stats.to_csv(f'data/analytics/reddit_subreddit_stats_{timestamp}.csv', encoding='utf-8-sig')
    
    print(f"\n‚úÖ Datos exportados con timestamp: {timestamp}")

# %%
# === RECOMENDACIONES ===
if not df_posts.empty:
    print("\n" + "="*60)
    print("üí° INSIGHTS DE REDDIT")
    print("="*60 + "\n")
    
    print("üéØ PRINCIPALES HALLAZGOS:\n")
    
    # Subreddit m√°s activo
    top_sub = subreddit_stats.index[0]
    print(f"1. Subreddit m√°s prometedor: r/{top_sub}")
    print(f"   - {subreddit_stats.loc[top_sub, 'oportunidades']:.0f} oportunidades detectadas")
    print(f"   - Score promedio: {subreddit_stats.loc[top_sub, 'score_promedio']:.1f}")
    
    # Palabras clave emergentes
    print(f"\n2. Top 5 palabras clave emergentes:")
    for i, row in top_words.head(5).iterrows():
        print(f"   - {row['palabra']} ({row['frecuencia']} menciones)")
    
    print("\n3. Pr√≥ximos pasos:")
    print("   - Monitorear r/Flipping y r/ThriftStoreHauls semanalmente")
    print("   - Crear alertas para keywords de alta demanda")
    print("   - Participar en comunidades para entender mejor las necesidades")
    
    print("\nüìã ACCI√ìN RECOMENDADA:")
    print("   Cruza estos datos con Mercado Libre y Google Trends")
    print("   para identificar el nicho con mejor score combinado")
