{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # üó£Ô∏è DataFlip MX - Reddit Sentiment Analysis\n",
    "\n",
    "\n",
    "\n",
    " **Objetivo:** Detectar quejas, necesidades y oportunidades en comunidades nicho\n",
    "\n",
    "\n",
    "\n",
    " **Docs:** https://praw.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTAR LIBRER√çAS ===\n",
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURACI√ìN DE REDDIT API ===\n",
    "# IMPORTANTE: Debes crear una app en https://www.reddit.com/prefs/apps\n",
    "\n",
    "# Para este ejemplo, usamos credenciales de placeholder\n",
    "# Reemplaza con tus credenciales reales en config/settings.py\n",
    "\n",
    "try:\n",
    "    from config.settings import REDDIT_CLIENT_ID, REDDIT_CLIENT_SECRET, REDDIT_USER_AGENT\n",
    "    \n",
    "    reddit = praw.Reddit(\n",
    "        client_id=REDDIT_CLIENT_ID,\n",
    "        client_secret=REDDIT_CLIENT_SECRET,\n",
    "        user_agent=REDDIT_USER_AGENT\n",
    "    )\n",
    "    \n",
    "    # Verificar conexi√≥n\n",
    "    print(f\"‚úÖ Conectado a Reddit como: {reddit.read_only}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error conectando a Reddit: {e}\")\n",
    "    print(\"   Configura tus credenciales en config/settings.py\")\n",
    "    reddit = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUBREDDITS RELEVANTES ===\n",
    "# Comunidades donde buscaremos oportunidades\n",
    "\n",
    "SUBREDDITS = [\n",
    "    'MechanicalKeyboards',  # Teclados mec√°nicos\n",
    "    'gamecollecting',       # Coleccionistas de videojuegos\n",
    "    'retrogaming',          # Gaming retro\n",
    "    'AnalogCommunity',      # Fotograf√≠a anal√≥gica\n",
    "    'Flipping',             # Comunidad de resellers\n",
    "    'ThriftStoreHauls',     # Hallazgos en segunda mano\n",
    "    'mexico',               # M√©xico general\n",
    "    'ITAM',                 # ITAM (si existe)\n",
    "]\n",
    "\n",
    "# Keywords a buscar\n",
    "KEYWORDS = [\n",
    "    'calculadora financiera',\n",
    "    'hp 12c',\n",
    "    'game boy',\n",
    "    'ipod',\n",
    "    'teclado mecanico',\n",
    "    'camara vintage',\n",
    "    'segunda mano',\n",
    "    'donde comprar',\n",
    "    'recomendaciones',\n",
    "]\n",
    "\n",
    "print(f\"üéØ {len(SUBREDDITS)} subreddits a analizar\")\n",
    "print(f\"üîç {len(KEYWORDS)} keywords a buscar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FUNCIONES DE SCRAPING ===\n",
    "\n",
    "def search_subreddit(subreddit_name: str, query: str, limit: int = 100, time_filter: str = 'year'):\n",
    "    \"\"\"\n",
    "    Busca posts en un subreddit espec√≠fico\n",
    "    \n",
    "    Args:\n",
    "        subreddit_name: Nombre del subreddit\n",
    "        query: T√©rmino de b√∫squeda\n",
    "        limit: N√∫mero m√°ximo de posts\n",
    "        time_filter: 'hour', 'day', 'week', 'month', 'year', 'all'\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con datos de posts\n",
    "    \"\"\"\n",
    "    if reddit is None:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        posts = []\n",
    "        \n",
    "        for submission in subreddit.search(query, limit=limit, time_filter=time_filter):\n",
    "            posts.append({\n",
    "                'id': submission.id,\n",
    "                'title': submission.title,\n",
    "                'selftext': submission.selftext,\n",
    "                'score': submission.score,\n",
    "                'num_comments': submission.num_comments,\n",
    "                'created_utc': datetime.fromtimestamp(submission.created_utc),\n",
    "                'author': str(submission.author),\n",
    "                'subreddit': subreddit_name,\n",
    "                'url': submission.url,\n",
    "                'permalink': f\"https://reddit.com{submission.permalink}\",\n",
    "                'keyword': query\n",
    "            })\n",
    "        \n",
    "        return posts\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en r/{subreddit_name} con '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "def get_top_posts(subreddit_name: str, limit: int = 50, time_filter: str = 'month'):\n",
    "    \"\"\"\n",
    "    Obtiene los posts m√°s populares de un subreddit\n",
    "    \"\"\"\n",
    "    if reddit is None:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        posts = []\n",
    "        \n",
    "        for submission in subreddit.top(limit=limit, time_filter=time_filter):\n",
    "            posts.append({\n",
    "                'id': submission.id,\n",
    "                'title': submission.title,\n",
    "                'selftext': submission.selftext[:500],  # Primeros 500 chars\n",
    "                'score': submission.score,\n",
    "                'num_comments': submission.num_comments,\n",
    "                'created_utc': datetime.fromtimestamp(submission.created_utc),\n",
    "                'author': str(submission.author),\n",
    "                'subreddit': subreddit_name,\n",
    "                'permalink': f\"https://reddit.com{submission.permalink}\"\n",
    "            })\n",
    "        \n",
    "        return posts\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en r/{subreddit_name}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AN√ÅLISIS: B√öSQUEDA POR KEYWORDS ===\n",
    "if reddit:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç BUSCANDO KEYWORDS EN SUBREDDITS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    all_posts = []\n",
    "    \n",
    "    # Buscar solo en subreddits m√°s relevantes para no saturar\n",
    "    priority_subreddits = ['Flipping', 'ThriftStoreHauls', 'mexico']\n",
    "    priority_keywords = ['game boy', 'calculadora', 'vintage']\n",
    "    \n",
    "    for subreddit in priority_subreddits:\n",
    "        for keyword in priority_keywords:\n",
    "            print(f\"üîç r/{subreddit} + '{keyword}'\")\n",
    "            posts = search_subreddit(subreddit, keyword, limit=25)\n",
    "            \n",
    "            if posts:\n",
    "                all_posts.extend(posts)\n",
    "                print(f\"   ‚úÖ {len(posts)} posts encontrados\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Sin resultados\")\n",
    "            \n",
    "            time.sleep(2)  # Rate limiting\n",
    "    \n",
    "    if all_posts:\n",
    "        df_posts = pd.DataFrame(all_posts)\n",
    "        print(f\"\\n‚úÖ Total: {len(df_posts)} posts recopilados\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No se encontraron posts\")\n",
    "        df_posts = pd.DataFrame()\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Saltando an√°lisis (Reddit API no configurada)\")\n",
    "    df_posts = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AN√ÅLISIS DE SENTIMIENTO B√ÅSICO ===\n",
    "if not df_posts.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé≠ AN√ÅLISIS DE SENTIMIENTO (B√ÅSICO)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Palabras clave de necesidad/oportunidad\n",
    "    oportunity_keywords = [\n",
    "        'no encuentro', 'donde comprar', 'alguien sabe', 'recomendaciones',\n",
    "        'busco', 'necesito', 'ayuda', 'd√≥nde', 'mejor', 'barato',\n",
    "        'vale la pena', 'worth it', 'looking for', 'recommend', 'help'\n",
    "    ]\n",
    "    \n",
    "    def detect_opportunity(text):\n",
    "        \"\"\"Detecta si un post expresa una necesidad/oportunidad\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        return any(keyword in text_lower for keyword in oportunity_keywords)\n",
    "    \n",
    "    # Combinar t√≠tulo y texto\n",
    "    df_posts['full_text'] = df_posts['title'] + ' ' + df_posts['selftext'].fillna('')\n",
    "    df_posts['is_opportunity'] = df_posts['full_text'].apply(detect_opportunity)\n",
    "    \n",
    "    # Filtrar oportunidades\n",
    "    df_opportunities = df_posts[df_posts['is_opportunity'] == True].copy()\n",
    "    \n",
    "    print(f\"üí° {len(df_opportunities)} posts identificados como OPORTUNIDADES\")\n",
    "    print(f\"   ({len(df_opportunities)/len(df_posts)*100:.1f}% del total)\\n\")\n",
    "    \n",
    "    # Mostrar top oportunidades\n",
    "    if not df_opportunities.empty:\n",
    "        top_opps = df_opportunities.nlargest(10, 'score')[['title', 'score', 'num_comments', 'subreddit', 'permalink']]\n",
    "        print(\"üî• Top 10 Oportunidades (por score):\\n\")\n",
    "        for idx, row in top_opps.iterrows():\n",
    "            print(f\"üìå {row['title'][:80]}...\")\n",
    "            print(f\"   ‚¨ÜÔ∏è  {row['score']} | üí¨ {row['num_comments']} | r/{row['subreddit']}\")\n",
    "            print(f\"   üîó {row['permalink']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AN√ÅLISIS DE FRECUENCIA DE PALABRAS ===\n",
    "if not df_posts.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä PALABRAS M√ÅS FRECUENTES\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    def clean_text(text):\n",
    "        \"\"\"Limpia y tokeniza texto\"\"\"\n",
    "        # Lowercase y eliminar caracteres especiales\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
    "        # Dividir en palabras\n",
    "        words = text.split()\n",
    "        # Filtrar stopwords b√°sicas\n",
    "        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                     'of', 'is', 'it', 'that', 'this', 'with', 'as', 'be', 'are', 'i', 'my',\n",
    "                     'de', 'la', 'el', 'en', 'y', 'que', 'por', 'para', 'con', 'un', 'una'}\n",
    "        words = [w for w in words if len(w) > 3 and w not in stopwords]\n",
    "        return words\n",
    "    \n",
    "    # Procesar todo el texto\n",
    "    all_words = []\n",
    "    for text in df_posts['full_text']:\n",
    "        all_words.extend(clean_text(str(text)))\n",
    "    \n",
    "    # Contar frecuencias\n",
    "    word_freq = Counter(all_words)\n",
    "    top_words = pd.DataFrame(word_freq.most_common(30), columns=['palabra', 'frecuencia'])\n",
    "    \n",
    "    print(top_words.head(20))\n",
    "    \n",
    "    # Visualizar\n",
    "    fig = px.bar(\n",
    "        top_words.head(20),\n",
    "        x='frecuencia',\n",
    "        y='palabra',\n",
    "        orientation='h',\n",
    "        title='üìä Top 20 Palabras M√°s Mencionadas',\n",
    "        labels={'frecuencia': 'Frecuencia', 'palabra': 'Palabra'},\n",
    "        height=600,\n",
    "        color='frecuencia',\n",
    "        color_continuous_scale='Teal'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        yaxis={'categoryorder':'total ascending'},\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AN√ÅLISIS POR SUBREDDIT ===\n",
    "if not df_posts.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà AN√ÅLISIS POR SUBREDDIT\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    subreddit_stats = df_posts.groupby('subreddit').agg({\n",
    "        'id': 'count',\n",
    "        'score': ['mean', 'max'],\n",
    "        'num_comments': ['mean', 'max'],\n",
    "        'is_opportunity': 'sum'\n",
    "    }).round(2)\n",
    "    \n",
    "    subreddit_stats.columns = ['_'.join(col).strip() for col in subreddit_stats.columns.values]\n",
    "    subreddit_stats = subreddit_stats.rename(columns={\n",
    "        'id_count': 'total_posts',\n",
    "        'score_mean': 'score_promedio',\n",
    "        'score_max': 'score_max',\n",
    "        'num_comments_mean': 'comentarios_promedio',\n",
    "        'num_comments_max': 'comentarios_max',\n",
    "        'is_opportunity_sum': 'oportunidades'\n",
    "    })\n",
    "    \n",
    "    subreddit_stats = subreddit_stats.sort_values('oportunidades', ascending=False)\n",
    "    \n",
    "    print(subreddit_stats)\n",
    "    \n",
    "    # Visualizar\n",
    "    fig = px.scatter(\n",
    "        subreddit_stats.reset_index(),\n",
    "        x='total_posts',\n",
    "        y='oportunidades',\n",
    "        size='score_promedio',\n",
    "        color='comentarios_promedio',\n",
    "        hover_name='subreddit',\n",
    "        title='üéØ Subreddits: Posts vs Oportunidades',\n",
    "        labels={\n",
    "            'total_posts': 'Total de Posts',\n",
    "            'oportunidades': 'Posts con Oportunidades',\n",
    "            'score_promedio': 'Score Promedio',\n",
    "            'comentarios_promedio': 'Comentarios Promedio'\n",
    "        },\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(template='plotly_white')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TENDENCIAS TEMPORALES ===\n",
    "if not df_posts.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìÖ ACTIVIDAD TEMPORAL\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Agregar columna de mes\n",
    "    df_posts['mes'] = df_posts['created_utc'].dt.to_period('M')\n",
    "    \n",
    "    # Contar posts por mes\n",
    "    temporal = df_posts.groupby('mes').agg({\n",
    "        'id': 'count',\n",
    "        'is_opportunity': 'sum'\n",
    "    }).rename(columns={'id': 'total_posts', 'is_opportunity': 'oportunidades'})\n",
    "    \n",
    "    temporal.index = temporal.index.to_timestamp()\n",
    "    \n",
    "    # Visualizar\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=temporal.index,\n",
    "        y=temporal['total_posts'],\n",
    "        name='Total Posts',\n",
    "        line=dict(color='blue', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=temporal.index,\n",
    "        y=temporal['oportunidades'],\n",
    "        name='Oportunidades',\n",
    "        line=dict(color='red', width=2, dash='dash')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='üìÖ Actividad en Reddit a lo Largo del Tiempo',\n",
    "        xaxis_title='Fecha',\n",
    "        yaxis_title='N√∫mero de Posts',\n",
    "        height=400,\n",
    "        template='plotly_white',\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPORTAR DATOS ===\n",
    "if not df_posts.empty:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Exportar posts completos\n",
    "    df_posts.to_csv(f'data/processed/reddit_posts_{timestamp}.csv', index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # Exportar solo oportunidades\n",
    "    if not df_opportunities.empty:\n",
    "        df_opportunities.to_csv(f'data/analytics/reddit_opportunities_{timestamp}.csv', index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    # Exportar estad√≠sticas de subreddits\n",
    "    subreddit_stats.to_csv(f'data/analytics/reddit_subreddit_stats_{timestamp}.csv', encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Datos exportados con timestamp: {timestamp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RECOMENDACIONES ===\n",
    "if not df_posts.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üí° INSIGHTS DE REDDIT\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    print(\"üéØ PRINCIPALES HALLAZGOS:\\n\")\n",
    "    \n",
    "    # Subreddit m√°s activo\n",
    "    top_sub = subreddit_stats.index[0]\n",
    "    print(f\"1. Subreddit m√°s prometedor: r/{top_sub}\")\n",
    "    print(f\"   - {subreddit_stats.loc[top_sub, 'oportunidades']:.0f} oportunidades detectadas\")\n",
    "    print(f\"   - Score promedio: {subreddit_stats.loc[top_sub, 'score_promedio']:.1f}\")\n",
    "    \n",
    "    # Palabras clave emergentes\n",
    "    print(f\"\\n2. Top 5 palabras clave emergentes:\")\n",
    "    for i, row in top_words.head(5).iterrows():\n",
    "        print(f\"   - {row['palabra']} ({row['frecuencia']} menciones)\")\n",
    "    \n",
    "    print(\"\\n3. Pr√≥ximos pasos:\")\n",
    "    print(\"   - Monitorear r/Flipping y r/ThriftStoreHauls semanalmente\")\n",
    "    print(\"   - Crear alertas para keywords de alta demanda\")\n",
    "    print(\"   - Participar en comunidades para entender mejor las necesidades\")\n",
    "    \n",
    "    print(\"\\nüìã ACCI√ìN RECOMENDADA:\")\n",
    "    print(\"   Cruza estos datos con Mercado Libre y Google Trends\")\n",
    "    print(\"   para identificar el nicho con mejor score combinado\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
